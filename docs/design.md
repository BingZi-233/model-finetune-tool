# ğŸ“˜ è®¾è®¡æ‰‹å†Œ

> model-finetune-tool v0.1.0 è®¾è®¡æ–‡æ¡£

## ç›®å½•

- [1. é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
- [2. ç³»ç»Ÿæ¶æ„](#2-ç³»ç»Ÿæ¶æ„)
- [3. æ¨¡å—è®¾è®¡](#3-æ¨¡å—è®¾è®¡)
- [4. æ•°æ®æµè®¾è®¡](#4-æ•°æ®æµè®¾è®¡)
- [5. æ•°æ®åº“è®¾è®¡](#5-æ•°æ®åº“è®¾è®¡)
- [6. APIè®¾è®¡](#6-apiè®¾è®¡)
- [7. é…ç½®è¯´æ˜](#7-é…ç½®è¯´æ˜)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 é¡¹ç›®èƒŒæ™¯

**model-finetune-tool** æ˜¯ä¸€ä¸ªä¾¿æ·çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒå·¥å…·ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·ï¼š

- ğŸ“„ **è§£æå¤šç§æ ¼å¼æ–‡æ¡£** - æ”¯æŒWordã€PDFã€Markdown
- ğŸ¤– **è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®** - åˆ©ç”¨LLMç”Ÿæˆé«˜è´¨é‡QAå¯¹
- ğŸ’¾ **é«˜æ•ˆæ•°æ®ç®¡ç†** - SQLiteç¼“å­˜ï¼Œæ”¯æŒMySQL/PostgreSQL
- âš¡ **ç®€åŒ–å¾®è°ƒæµç¨‹** - åŸºäºLoRAçš„é«˜æ•ˆå¾®è°ƒ

### 1.2 è®¾è®¡ç†å¿µ

| åŸåˆ™ | æè¿° |
|------|------|
| **æ¨¡å—åŒ–** | å„æ¨¡å—èŒè´£å•ä¸€ï¼Œå¯ç‹¬ç«‹ä½¿ç”¨ |
| **å¯æ‰©å±•** | æ˜“äºæ·»åŠ æ–°çš„æ–‡æ¡£è§£æå™¨æˆ–LLMæä¾›å•† |
| **æ˜“ç”¨æ€§** | CLIäº¤äº’ï¼Œé…ç½®ç®€å• |
| **å¯é æ€§** | å®Œå–„çš„æµ‹è¯•è¦†ç›– |

### 1.3 æŠ€æœ¯æ ˆ

```
Python 3.10+
â”œâ”€â”€ pydantic        - é…ç½®ç®¡ç†
â”œâ”€â”€ pyyaml          - YAMLé…ç½®
â”œâ”€â”€ openai          - LLMè°ƒç”¨
â”œâ”€â”€ python-docx     - Wordè§£æ
â”œâ”€â”€ pymupdf         - PDFè§£æ
â”œâ”€â”€ sqlalchemy      - æ•°æ®åº“ORM
â”œâ”€â”€ peft            - LoRAå¾®è°ƒ
â”œâ”€â”€ transformers    - æ¨¡å‹åŠ è½½
â””â”€â”€ click           - CLIæ¡†æ¶
```

---

## 2. ç³»ç»Ÿæ¶æ„

### 2.1 æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph ç”¨æˆ·å±‚
        CLI[å‘½ä»¤è¡Œç•Œé¢]
        Config[YAMLé…ç½®]
    end
    
    subgraph æ ¸å¿ƒå±‚
        Parser[æ–‡æ¡£è§£æå™¨]
        LLM[LLMå®¢æˆ·ç«¯]
        Dataset[æ•°æ®é›†ç®¡ç†]
        Trainer[è®­ç»ƒæ¨¡å—]
    end
    
    subgraph æ•°æ®å±‚
        SQLite[(SQLiteæ•°æ®åº“)]
        Files[æ–‡æ¡£æ–‡ä»¶]
        Cache[LLMå“åº”ç¼“å­˜]
    end
    
    subgraph å¤–éƒ¨æœåŠ¡
        OpenAI[OpenAI API]
        HF[HuggingFace]
    end
    
    CLI --> Config
    CLI --> Parser
    CLI --> LLM
    CLI --> Dataset
    CLI --> Trainer
    
    Parser --> Files
    Parser --> Dataset
    
    LLM --> OpenAI
    LLM --> Cache
    LLM --> Dataset
    
    Dataset --> SQLite
    Trainer --> HF
    Trainer --> Dataset
```

### 2.2 ç»„ä»¶è¯´æ˜

| ç»„ä»¶ | èŒè´£ | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|
| **ParserManager** | ç»Ÿä¸€æ–‡æ¡£è§£æ | æ–‡ä»¶è·¯å¾„ | æ–‡æœ¬æ®µè½åˆ—è¡¨ |
| **LLMClient** | è°ƒç”¨LLMç”Ÿæˆæ•°æ® | æ–‡æœ¬å†…å®¹ | QAå¯¹/æ‘˜è¦ |
| **DatasetManager** | æ•°æ®é›†CRUDç®¡ç† | æ•°æ®æ¡ç›® | æ•°æ®åº“è®°å½• |
| **Trainer** | æ¨¡å‹è®­ç»ƒä¸åˆå¹¶ | è®­ç»ƒæ•°æ® | LoRAæ¨¡å‹/åˆå¹¶æ¨¡å‹ |

---

## 3. æ¨¡å—è®¾è®¡

### 3.1 æ–‡æ¡£è§£ææ¨¡å— (parser)

#### 3.1.1 ç±»å›¾

```mermaid
classDiagram
    class BaseParser {
        <<abstract>>
        +parse(file_path) List~str~
        +supports(file_path) bool
    }
    
    class DocxParser {
        +supports(file_path) bool
        +parse(file_path) List~str~
    }
    
    class PdfParser {
        +supports(file_path) bool
        +parse(file_path) List~str~
    }
    
    class MarkdownParser {
        +supports(file_path) bool
        +parse(file_path) List~str~
    }
    
    class ParserManager {
        +parsers: List~BaseParser~
        +parse_file(file_path) List~str~
        +parse_directory(dir_path) Dict
        +get_supported_extensions() List~str~
    }
    
    BaseParser <|-- DocxParser
    BaseParser <|-- PdfParser
    BaseParser <|-- MarkdownParser
    ParserManager --> BaseParser
```

#### 3.1.2 è§£æå™¨ç­–ç•¥

| è§£æå™¨ | æ”¯æŒæ ¼å¼ | ä¾èµ–åº“ | ç‰¹ç‚¹ |
|--------|----------|--------|------|
| DocxParser | .docx | python-docx | æå–æ®µè½+è¡¨æ ¼ |
| PdfParser | .pdf | PyMuDF | æŒ‰é¡µè§£ææ–‡æœ¬ |
| MarkdownParser | .md | æ ‡å‡†åº“ | æ¸…ç†æ ¼å¼ç¬¦å· |

### 3.2 LLMè°ƒç”¨æ¨¡å— (llm)

#### 3.2.1 ç±»å›¾

```mermaid
classDiagram
    class LLMClient {
        -client: OpenAI
        +chat(messages) str
        +generate_qa_pairs(text, num_pairs) List~Dict~
        +generate_summarization(text) str
        +batch_generate_qa(texts) List~Dict~
        -_extract_json(response) List~Dict~
    }
    
    class CacheManager {
        -cache_dir: Path
        +set(text, response)
        +get(text) str
        -_get_cache_key(text) str
    }
    
    LLMClient --> CacheManager
```

#### 3.2.2 QAç”Ÿæˆæµç¨‹

```
åŸå§‹æ–‡æ¡£æ–‡æœ¬
    â†“
æ–‡æœ¬åˆ†å—å¤„ç†
    â†“
è°ƒç”¨LLM (GPT-3.5/GPT-4)
    â†“
æå–JSONå“åº”
    â†“
æ ¼å¼éªŒè¯
    â†“
QAå¯¹åˆ—è¡¨
```

### 3.3 æ•°æ®é›†ç®¡ç†æ¨¡å— (dataset)

#### 3.3.1 ç±»å›¾

```mermaid
classDiagram
    class Document {
        +id: int
        +file_path: str
        +file_name: str
        +file_type: str
        +content_hash: str
        +created_at: DateTime
    }
    
    class DatasetItem {
        +id: int
        +dataset_name: str
        +instruction: str
        +input: str
        +output: str
        +document_id: int
        +chunk_index: int
        +source_file: str
    }
    
    class DatasetManager {
        +engine: Engine
        +add_document(file_path, hash, metadata)
        +add_dataset_item(...) id
        +get_dataset_items(name) List~DatasetItem~
        +export_dataset(name) List~Dict~
        +save_to_jsonl(name, path) int
    }
    
    DatasetManager --> Document
    DatasetManager --> DatasetItem
```

### 3.4 è®­ç»ƒæ¨¡å— (trainer)

#### 3.4.1 è®­ç»ƒæµç¨‹å›¾

```mermaid
flowchart TD
    A[JSONLæ•°æ®é›†] --> B[æ•°æ®æ ¼å¼åŒ–]
    B --> C{chat_template}
    C -->|Yes| D[è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼]
    C -->|No| E[è½¬æ¢ä¸ºæŒ‡ä»¤æ ¼å¼]
    D --> F[Tokenize]
    E --> F
    F --> G[åŠ è½½é¢„è®­ç»ƒæ¨¡å‹]
    G --> H[é…ç½®LoRA]
    H --> I[è®­ç»ƒ]
    I --> J[ä¿å­˜LoRAæƒé‡]
```

---

## 4. æ•°æ®æµè®¾è®¡

### 4.1 ç«¯åˆ°ç«¯æ•°æ®æµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ•°æ®æµå¤„ç†æµç¨‹                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. æ–‡æ¡£è¾“å…¥é˜¶æ®µ                                                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚     â”‚ .docx    â”‚â”€â”€â”€â–¶â”‚ Parser   â”‚â”€â”€â”€â–¶â”‚ æ–‡æœ¬æ®µè½ â”‚                  â”‚
â”‚     â”‚ .pdf     â”‚    â”‚ Manager  â”‚    â”‚   åˆ—è¡¨   â”‚                  â”‚
â”‚     â”‚ .md      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚
â”‚                                                                     â”‚
â”‚  2. æ•°æ®ç”Ÿæˆé˜¶æ®µ                                                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚     â”‚ æ–‡æœ¬æ®µè½ â”‚â”€â”€â”€â–¶â”‚  LLM     â”‚â”€â”€â”€â–¶â”‚  QAå¯¹    â”‚                  â”‚
â”‚     â”‚   åˆ—è¡¨   â”‚    â”‚  Client  â”‚    â”‚   åˆ—è¡¨   â”‚                  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                   â”‚                â”‚
â”‚                                                   â–¼                â”‚
â”‚                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                                              â”‚  SQLite  â”‚         â”‚
â”‚                                              â”‚   æ•°æ®åº“  â”‚         â”‚
â”‚                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                     â”‚
â”‚  3. è®­ç»ƒé˜¶æ®µ                                                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚     â”‚  SQLite  â”‚â”€â”€â”€â–¶â”‚ å¯¼å‡ºJSONLâ”‚â”€â”€â”€â–¶â”‚  æ ¼å¼åŒ–  â”‚                  â”‚
â”‚     â”‚   æ•°æ®åº“  â”‚    â”‚   æ–‡ä»¶   â”‚    â”‚   æ•°æ®   â”‚                  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                   â”‚                â”‚
â”‚                                                   â–¼                â”‚
â”‚                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                                              â”‚  è®­ç»ƒ   â”‚         â”‚
â”‚                                              â”‚ LoRAæ¨¡å‹ â”‚         â”‚
â”‚                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                     â”‚
â”‚  4. è¾“å‡ºé˜¶æ®µ                                                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚     â”‚ LoRAæƒé‡ â”‚â”€â”€â”€â–¶â”‚  æ¨¡å‹    â”‚â”€â”€â”€â–¶â”‚ å¾®è°ƒå   â”‚                  â”‚
â”‚     â”‚   æ–‡ä»¶   â”‚    â”‚   åˆå¹¶   â”‚    â”‚   æ¨¡å‹   â”‚                  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 æ•°æ®æ ¼å¼

#### 4.2.1 è®­ç»ƒæ•°æ®æ ¼å¼

```json
// å•æ¡è®­ç»ƒæ•°æ®
{
  "instruction": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
  "input": "",
  "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯..."
}

// èŠå¤©æ ¼å¼
{
  "messages": [
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"},
    {"role": "assistant", "content": "æœºå™¨å­¦ä¹ æ˜¯..."}
  ]
}
```

#### 4.2.2 é…ç½®æ–‡ä»¶æ ¼å¼

```yaml
# å®Œæ•´é…ç½®ç¤ºä¾‹
llm:
  api_key: "${OPENAI_API_KEY}"
  base_url: "https://api.openai.com/v1"
  model: "gpt-3.5-turbo"
  temperature: 0.7
  max_tokens: 2000

database:
  type: "sqlite"
  path: "./data/datasets.db"

datasets:
  input_dir: "./documents"
  chunk_size: 1000
  chunk_overlap: 200

training:
  model_name: "Qwen/Qwen2.5-0.5B-Instruct"
  lora:
    r: 8
    alpha: 16
    dropout: 0.1
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  batch_size: 4
  learning_rate: 0.0002
  epochs: 3
```

---

## 5. æ•°æ®åº“è®¾è®¡

### 5.1 ERå›¾

```mermaid
erDiagram
    documents {
        int id PK "ä¸»é”®"
        string file_path UK "æ–‡ä»¶è·¯å¾„"
        string file_name "æ–‡ä»¶å"
        string file_type "æ–‡ä»¶ç±»å‹"
        string content_hash "å†…å®¹å“ˆå¸Œ"
        datetime created_at "åˆ›å»ºæ—¶é—´"
        json metadata "å…ƒæ•°æ®"
    }
    
    dataset_items {
        int id PK "ä¸»é”®"
        string dataset_name "æ•°æ®é›†åç§°"
        int document_id FK "æ–‡æ¡£ID"
        text instruction "æŒ‡ä»¤"
        text input "è¾“å…¥"
        text output "è¾“å‡º"
        int chunk_index "å—ç´¢å¼•"
        string source_file "æºæ–‡ä»¶"
        datetime created_at "åˆ›å»ºæ—¶é—´"
        json metadata "å…ƒæ•°æ®"
    }
    
    documents ||--o{ dataset_items : "contains"
```

### 5.2 è¡¨ç»“æ„è¯´æ˜

#### 5.2.1 documents è¡¨

| å­—æ®µ | ç±»å‹ | çº¦æŸ | è¯´æ˜ |
|------|------|------|------|
| id | INTEGER | PRIMARY KEY | è‡ªå¢ä¸»é”® |
| file_path | VARCHAR(500) | UNIQUE, NOT NULL | æ–‡ä»¶è·¯å¾„ |
| file_name | VARCHAR(255) | NOT NULL | æ–‡ä»¶å |
| file_type | VARCHAR(50) | NOT NULL | æ–‡ä»¶ç±»å‹(.docx/.pdf/.md) |
| content_hash | VARCHAR(64) | NOT NULL | å†…å®¹å“ˆå¸Œå€¼ |
| created_at | DATETIME | DEFAULT | åˆ›å»ºæ—¶é—´ |
| metadata | JSON | NULL | é¢å¤–å…ƒæ•°æ® |

#### 5.2.2 dataset_items è¡¨

| å­—æ®µ | ç±»å‹ | çº¦æŸ | è¯´æ˜ |
|------|------|------|------|
| id | INTEGER | PRIMARY KEY | è‡ªå¢ä¸»é”® |
| dataset_name | VARCHAR(100) | NOT NULL, INDEX | æ•°æ®é›†åç§° |
| document_id | INTEGER | FOREIGN KEY | å…³è”æ–‡æ¡£ |
| instruction | TEXT | NOT NULL | é—®é¢˜/æŒ‡ä»¤ |
| input | TEXT | NULL | è¾“å…¥å†…å®¹ |
| output | TEXT | NULL | è¾“å‡ºç­”æ¡ˆ |
| chunk_index | INTEGER | DEFAULT | æ–‡æœ¬å—ç´¢å¼• |
| source_file | VARCHAR(500) | NULL | æºæ–‡ä»¶è·¯å¾„ |
| created_at | DATETIME | DEFAULT | åˆ›å»ºæ—¶é—´ |
| metadata | JSON | NULL | é¢å¤–å…ƒæ•°æ® |

---

## 6. APIè®¾è®¡

### 6.1 CLIå‘½ä»¤

| å‘½ä»¤ | å‚æ•° | è¯´æ˜ |
|------|------|------|
| `finetune init` | [--config] | åˆå§‹åŒ–é¡¹ç›® |
| `finetune parse` | <input_dir> <dataset_name> | è§£ææ–‡æ¡£ç”Ÿæˆæ•°æ®é›† |
| `finetune export` | <dataset_name> [--format] [--output] | å¯¼å‡ºæ•°æ®é›† |
| `finetune stats` | <dataset_name> | æŸ¥çœ‹æ•°æ®é›†ç»Ÿè®¡ |
| `finetune train` | <dataset_name> [--model] [--epochs] | è®­ç»ƒæ¨¡å‹ |
| `finetune merge` | <dataset_name> <base_model> | åˆå¹¶æ¨¡å‹ |
| `finetune clear` | <dataset_name> | æ¸…ç©ºæ•°æ®é›† |

### 6.2 æ¨¡å—API

#### 6.2.1 ParserManager

```python
class ParserManager:
    def parse_file(self, file_path: str) -> List[str]:
        """è§£æå•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            æ–‡æœ¬æ®µè½åˆ—è¡¨
        """
        pass
    
    def parse_directory(self, dir_path: str, recursive: bool = True) -> Dict[str, List[str]]:
        """è§£ææ•´ä¸ªç›®å½•
        
        Args:
            dir_path: ç›®å½•è·¯å¾„
            recursive: æ˜¯å¦é€’å½’å¤„ç†å­ç›®å½•
        
        Returns:
            æ–‡ä»¶è·¯å¾„åˆ°æ®µè½åˆ—è¡¨çš„æ˜ å°„
        """
        pass
```

#### 6.2.2 DatasetManager

```python
class DatasetManager:
    def add_document(self, file_path: str, content_hash: str, metadata: Dict = None) -> int:
        """æ·»åŠ æ–‡æ¡£è®°å½•"""
        pass
    
    def add_dataset_item(
        self,
        dataset_name: str,
        instruction: str,
        input_: str = None,
        output: str = None,
        **kwargs
    ) -> int:
        """æ·»åŠ æ•°æ®é›†æ¡ç›®"""
        pass
    
    def export_dataset(self, dataset_name: str) -> List[Dict]:
        """å¯¼å‡ºæ•°æ®é›†"""
        pass
```

---

## 7. é…ç½®è¯´æ˜

### 7.1 é…ç½®ä¼˜å…ˆçº§

```
å‘½ä»¤è¡Œå‚æ•° > ç¯å¢ƒå˜é‡ > é…ç½®æ–‡ä»¶ > é»˜è®¤å€¼
```

### 7.2 ç¯å¢ƒå˜é‡

| å˜é‡ | è¯´æ˜ |
|------|------|
| `OPENAI_API_KEY` | OpenAI APIå¯†é’¥ |
| `OPENAI_BASE_URL` | APIåŸºç¡€URL (å¯é€‰) |

### 7.3 å®Œæ•´é…ç½®ç¤ºä¾‹

```yaml
# model-finetune-tool å®Œæ•´é…ç½®

# =====================================
# LLMé…ç½®
# =====================================
llm:
  api_key: "${OPENAI_API_KEY}"        # ä½¿ç”¨ç¯å¢ƒå˜é‡
  base_url: "https://api.openai.com/v1"
  model: "gpt-3.5-turbo"              # å¯é€‰: gpt-4, deepseek-chatç­‰
  temperature: 0.7                     # ç”Ÿæˆæ¸©åº¦ (0-2)
  max_tokens: 2000                     # æœ€å¤§ç”Ÿæˆtokenæ•°

# =====================================
# æ•°æ®åº“é…ç½®
# =====================================
database:
  type: "sqlite"                       # sqlite | mysql | postgresql
  path: "./data/datasets.db"           # SQLiteæ–‡ä»¶è·¯å¾„
  # MySQLé…ç½®
  host: "localhost"
  port: 3306
  username: "root"
  password: "your_password"
  database: "model_finetune"
  # PostgreSQLé…ç½®
  # host: "localhost"
  # port: 5432
  # username: "postgres"
  # password: "your_password"
  # database: "model_finetune"

# =====================================
# æ•°æ®é›†é…ç½®
# =====================================
datasets:
  input_dir: "./documents"             # æ–‡æ¡£ç›®å½•
  output_dir: "./data"                 # è¾“å‡ºç›®å½•
  chunk_size: 1000                     # æ–‡æœ¬å—å¤§å°
  chunk_overlap: 200                   # å—é‡å å¤§å°

# =====================================
# è®­ç»ƒé…ç½®
# =====================================
training:
  model_name: "Qwen/Qwen2.5-0.5B-Instruct"  # æ¨¡å‹åç§°
  lora:
    r: 8                               # LoRA rank
    alpha: 16                          # LoRA alpha
    dropout: 0.1                       # Dropoutæ¯”ä¾‹
    target_modules:                    # ç›®æ ‡æ¨¡å—
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
  batch_size: 4                        # æ‰¹æ¬¡å¤§å°
  learning_rate: 0.0002               # å­¦ä¹ ç‡
  epochs: 3                            # è®­ç»ƒè½®æ•°
  max_length: 2048                    # æœ€å¤§åºåˆ—é•¿åº¦

# =====================================
# è¾“å‡ºé…ç½®
# =====================================
output:
  model_dir: "./output"               # æ¨¡å‹è¾“å‡ºç›®å½•
  checkpoint_dir: "./checkpoints"     # æ£€æŸ¥ç‚¹ç›®å½•

# =====================================
# Gité…ç½®
# =====================================
git:
  auto_commit: true                   # è‡ªåŠ¨æäº¤
  commit_message: "Update dataset: {dataset_name}"
```

---

## é™„å½•

### A. é”™è¯¯ç è¯´æ˜

| é”™è¯¯ç  | è¯´æ˜ |
|--------|------|
| 1001 | é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ |
| 1002 | é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ |
| 2001 | ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ |
| 2002 | æ–‡æ¡£è§£æå¤±è´¥ |
| 3001 | LLM APIè°ƒç”¨å¤±è´¥ |
| 3002 | LLMå“åº”è§£æå¤±è´¥ |
| 4001 | æ•°æ®åº“è¿æ¥å¤±è´¥ |
| 4002 | æ•°æ®æ“ä½œå¤±è´¥ |

### B. æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ‰¹é‡å¤„ç†** - å¤§é‡æ–‡æ¡£æ—¶ä½¿ç”¨æ‰¹é‡è§£æ
2. **ç¼“å­˜** - å¯ç”¨LLMå“åº”ç¼“å­˜é¿å…é‡å¤è°ƒç”¨
3. **å¹¶è¡Œ** - è€ƒè™‘å¤šè¿›ç¨‹å¤„ç†æ–‡æ¡£
4. **åˆ†å—** - å¤§æ–‡æ¡£åˆç†åˆ†å—å¤„ç†
