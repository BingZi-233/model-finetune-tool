# ğŸ“– ä½¿ç”¨æ‰‹å†Œ

> model-finetune-tool v0.1.0 ä½¿ç”¨æŒ‡å—

## ç›®å½•

- [1. å¿«é€Ÿå¼€å§‹](#1-å¿«é€Ÿå¼€å§‹)
- [2. ç¯å¢ƒå‡†å¤‡](#2-ç¯å¢ƒå‡†å¤‡)
- [3. å®‰è£…éƒ¨ç½²](#3-å®‰è£…éƒ¨ç½²)
- [4. é…ç½®è¯´æ˜](#4-é…ç½®è¯´æ˜)
- [5. ä½¿ç”¨æ•™ç¨‹](#5-ä½¿ç”¨æ•™ç¨‹)
- [6. å¸¸è§é—®é¢˜](#6-å¸¸è§é—®é¢˜)
- [7. æœ€ä½³å®è·µ](#7-æœ€ä½³å®è·µ)

---

## 1. å¿«é€Ÿå¼€å§‹

### 1.1 5åˆ†é’Ÿä¸Šæ‰‹

```bash
# 1. å…‹éš†å¹¶å®‰è£…
git clone https://github.com/yourname/model-finetune-tool.git
cd model-finetune-tool
pip install -e .

# 2. é…ç½®APIå¯†é’¥
export OPENAI_API_KEY="sk-xxx"

# 3. å‡†å¤‡æ–‡æ¡£
mkdir -p documents
# æŠŠä½ çš„docx/pdf/mdæ–‡ä»¶æ”¾å…¥documentsç›®å½•

# 4. è§£ææ–‡æ¡£ç”Ÿæˆæ•°æ®
finetune parse ./documents my_dataset

# 5. æŸ¥çœ‹æ•°æ®é›†
finetune stats my_dataset

# 6. è®­ç»ƒæ¨¡å‹
finetune train my_dataset
```

### 1.2 é¢„æœŸè¾“å‡º

```
âœ… è§£æå®Œæˆï¼å…±ç”Ÿæˆ 150 æ¡æ•°æ®
âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ°: ./output/my_dataset/lora_model
```

---

## 2. ç¯å¢ƒå‡†å¤‡

### 2.1 ç³»ç»Ÿè¦æ±‚

| è¦æ±‚ | æœ€ä½ç‰ˆæœ¬ | æ¨èç‰ˆæœ¬ |
|------|----------|----------|
| Python | 3.10 | 3.11+ |
| å†…å­˜ | 4GB | 16GB+ |
| ç£ç›˜ | 10GB | 50GB+ |
| GPU | å¯é€‰ | NVIDIA 8GB+ |

### 2.2 ç¡¬ä»¶é…ç½®å»ºè®®

| ä½¿ç”¨åœºæ™¯ | é…ç½® | è¯´æ˜ |
|----------|------|------|
| å­¦ä¹ /æµ‹è¯• | CPUå³å¯ | è§£ææ–‡æ¡£ã€ç”Ÿæˆæ•°æ® |
| å¾®è°ƒè®­ç»ƒ | GPU 8GB+ | Qwen2.5-0.5B å¯åœ¨æ¶ˆè´¹çº§GPUè¿è¡Œ |
| ç”Ÿäº§éƒ¨ç½² | GPU 16GB+ | å¯è¿è¡Œæ›´å¤§æ¨¡å‹ |

### 2.3 ä¾èµ–ç¯å¢ƒ

```bash
# Python 3.10+
python --version

# Git
git --version

# (å¯é€‰) CUDA (ç”¨äºGPUè®­ç»ƒ)
nvidia-smi
```

---

## 3. å®‰è£…éƒ¨ç½²

### 3.1 å…‹éš†é¡¹ç›®

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/yourname/model-finetune-tool.git
cd model-finetune-tool
```

### 3.2 å®‰è£…ä¾èµ–

#### æ–¹å¼ä¸€ï¼špip (æ¨è)

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
.\venv\Scripts\activate   # Windows

# å®‰è£…é¡¹ç›®
pip install -e .
```

#### æ–¹å¼äºŒï¼šPoetry

```bash
# å®‰è£…Poetry (å¦‚æœæœªå®‰è£…)
pip install poetry

# å®‰è£…ä¾èµ–
poetry install
```

### 3.3 éªŒè¯å®‰è£…

```bash
# æŸ¥çœ‹ç‰ˆæœ¬
finetune --version

# æŸ¥çœ‹å¸®åŠ©
finetune --help

# é¢„æœŸè¾“å‡ºï¼š
# Usage: finetune [OPTIONS] COMMAND [ARGS]...
# 
# Commands:
#   clear     æ¸…ç©ºæ•°æ®é›†
#   export    å¯¼å‡ºæ•°æ®é›†
#   init      åˆå§‹åŒ–é¡¹ç›®
#   merge     åˆå¹¶æ¨¡å‹
#   parse     è§£ææ–‡æ¡£
#   stats     æŸ¥çœ‹ç»Ÿè®¡
#   train     è®­ç»ƒæ¨¡å‹
```

---

## 4. é…ç½®è¯´æ˜

### 4.1 é…ç½®æ–‡ä»¶ä½ç½®

é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ `config.yaml`ï¼š

```bash
model-finetune-tool/
â”œâ”€â”€ config.yaml          # ä¸»é…ç½®æ–‡ä»¶
â”œâ”€â”€ pyproject.toml       # é¡¹ç›®é…ç½®
â””â”€â”€ ...
```

### 4.2 æœ€å°é…ç½®

åªéœ€è¦é…ç½®LLM APIå¯†é’¥å³å¯å¼€å§‹ï¼š

```yaml
llm:
  api_key: "${OPENAI_API_KEY}"  # è®¾ç½®ç¯å¢ƒå˜é‡OPENAI_API_KEY
```

### 4.3 å®Œæ•´é…ç½®

```yaml
# =====================================
# LLMé…ç½® (å¿…éœ€)
# =====================================
llm:
  api_key: "${OPENAI_API_KEY}"
  base_url: "https://api.openai.com/v1"
  model: "gpt-3.5-turbo"
  temperature: 0.7
  max_tokens: 2000

# =====================================
# æ•°æ®åº“é…ç½® (å¯é€‰)
# =====================================
database:
  type: "sqlite"
  path: "./data/datasets.db"

# =====================================
# æ•°æ®é›†é…ç½® (å¯é€‰)
# =====================================
datasets:
  input_dir: "./documents"
  chunk_size: 1000
  chunk_overlap: 200

# =====================================
# è®­ç»ƒé…ç½® (å¯é€‰)
# =====================================
training:
  model_name: "Qwen/Qwen2.5-0.5B-Instruct"
  lora:
    r: 8
    alpha: 16
    dropout: 0.1
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  batch_size: 4
  learning_rate: 0.0002
  epochs: 3
  max_length: 2048

# =====================================
# è¾“å‡ºé…ç½® (å¯é€‰)
# =====================================
output:
  model_dir: "./output"
  checkpoint_dir: "./checkpoints"

# =====================================
# Gité…ç½® (å¯é€‰)
# =====================================
git:
  auto_commit: true
  commit_message: "Update dataset: {dataset_name}"
```

### 4.4 é…ç½®é¡¹è¯¦è§£

#### LLMé…ç½®

| å‚æ•° | å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `api_key` | æ˜¯ | - | APIå¯†é’¥ï¼Œæ”¯æŒç¯å¢ƒå˜é‡ |
| `base_url` | å¦ | OpenAIå®˜æ–¹ | APIåŸºç¡€URL |
| `model` | å¦ | gpt-3.5-turbo | æ¨¡å‹åç§° |
| `temperature` | å¦ | 0.7 | ç”Ÿæˆæ¸©åº¦ (0-2) |
| `max_tokens` | å¦ | 2000 | æœ€å¤§ç”Ÿæˆé•¿åº¦ |

#### æ•°æ®åº“é…ç½®

| å‚æ•° | å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `type` | å¦ | sqlite | æ•°æ®åº“ç±»å‹: sqlite/mysql/postgresql |
| `path` | å¦ | ./data/datasets.db | SQLiteæ–‡ä»¶è·¯å¾„ |
| `host` | å¦ | localhost | MySQL/PostgreSQLä¸»æœº |
| `port` | å¦ | 3306/5432 | ç«¯å£ |
| `username` | å¦ | - | ç”¨æˆ·å |
| `password` | å¦ | - | å¯†ç  |
| `database` | å¦ | - | æ•°æ®åº“å |

#### è®­ç»ƒé…ç½®

| å‚æ•° | å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `model_name` | å¦ | Qwen2.5-0.5B | HuggingFaceæ¨¡å‹å |
| `lora.r` | å¦ | 8 | LoRA rank |
| `lora.alpha` | å¦ | 16 | LoRA alpha |
| `lora.dropout` | å¦ | 0.1 | Dropoutæ¯”ä¾‹ |
| `batch_size` | å¦ | 4 | æ‰¹æ¬¡å¤§å° |
| `learning_rate` | å¦ | 0.0002 | å­¦ä¹ ç‡ |
| `epochs` | å¦ | 3 | è®­ç»ƒè½®æ•° |

---

## 5. ä½¿ç”¨æ•™ç¨‹

### 5.1 æ•™ç¨‹ä¸€ï¼šåŸºç¡€ä½¿ç”¨æµç¨‹

#### æ­¥éª¤1ï¼šå‡†å¤‡æ–‡æ¡£

åˆ›å»º `documents` ç›®å½•ï¼Œæ”¾å…¥ä½ çš„æ–‡æ¡£ï¼š

```bash
mkdir -p documents

# æ”¾å…¥æ–‡æ¡£
cp /path/to/your/*.md documents/
cp /path/to/your/*.docx documents/
cp /path/to/your/*.pdf documents/
```

æ”¯æŒçš„æ ¼å¼ï¼š
- `.docx` - Wordæ–‡æ¡£
- `.pdf` - PDFæ–‡æ¡£  
- `.md` - Markdownæ–‡æ¡£

#### æ­¥éª¤2ï¼šé…ç½®APIå¯†é’¥

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export OPENAI_API_KEY="sk-your-api-key"

# æˆ–åœ¨config.yamlä¸­ç›´æ¥é…ç½®
# llm:
#   api_key: "sk-your-api-key"
```

#### æ­¥éª¤3ï¼šè§£ææ–‡æ¡£ç”Ÿæˆæ•°æ®

```bash
# åŸºæœ¬ç”¨æ³•
finetune parse ./documents my_dataset

# é«˜çº§ç”¨æ³•
finetune parse ./documents my_dataset \
    --recursive \
    --chunk-size 1500 \
    --qa-pairs 5

# å‚æ•°è¯´æ˜ï¼š
# --recursive    é€’å½’å¤„ç†å­ç›®å½•
# --chunk-size   æ–‡æœ¬å—å¤§å° (é»˜è®¤1000)
# --qa-pairs     æ¯å—ç”Ÿæˆçš„QAå¯¹æ•° (é»˜è®¤3)
```

é¢„æœŸè¾“å‡ºï¼š
```
è§£ææ–‡æ¡£: ./documents
æ‰¾åˆ° 10 ä¸ªæ–‡æ¡£
å¤„ç†æ–‡æ¡£: ./documents/test.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10
âœ… å®Œæˆï¼å…±ç”Ÿæˆ 150 æ¡æ•°æ®
```

#### æ­¥éª¤4ï¼šæŸ¥çœ‹æ•°æ®é›†ç»Ÿè®¡

```bash
finetune stats my_dataset
```

è¾“å‡ºï¼š
```
æ•°æ®é›†: my_dataset
æ€»æ¡ç›®: 150
```

#### æ­¥éª¤5ï¼šå¯¼å‡ºæ•°æ®ï¼ˆå¯é€‰ï¼‰

```bash
# å¯¼å‡ºä¸ºJSONL
finetune export my_dataset -o train.jsonl

# å¯¼å‡ºä¸ºJSON
finetune export my_dataset -o train.json --format json

# æŸ¥çœ‹å¯¼å‡ºçš„æ•°æ®
head -n 5 train.jsonl
```

#### æ­¥éª¤6ï¼šè®­ç»ƒæ¨¡å‹

```bash
# åŸºæœ¬ç”¨æ³•
finetune train my_dataset

# é«˜çº§ç”¨æ³•
finetune train my_dataset \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --epochs 5 \
    --batch-size 4

# å‚æ•°è¯´æ˜ï¼š
# --model     æ¨¡å‹åç§° (HuggingFace)
# --epochs    è®­ç»ƒè½®æ•°
# --batch-size æ‰¹æ¬¡å¤§å°
```

é¢„æœŸè¾“å‡ºï¼š
```
å¼€å§‹è®­ç»ƒæ¨¡å‹: Qwen/Qwen2.5-0.5B-Instruct
åŠ è½½æ¨¡å‹: Qwen/Qwen2.5-0.5B-Instruct
trainable params: 1,048,576 || all params: 487,616,000 || trainable%: 0.2150
å¼€å§‹è®­ç»ƒ...
Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 10/10
âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ°: ./output/my_dataset/lora_model
```

#### æ­¥éª¤7ï¼šåˆå¹¶æ¨¡å‹ï¼ˆå¯é€‰ï¼‰

```bash
# åˆå¹¶åŸºç¡€æ¨¡å‹å’ŒLoRA
finetune merge my_dataset Qwen/Qwen2.5-0.5B-Instruct

# è¾“å‡ºç›®å½•: ./output/my_dataset/merged
```

### 5.2 æ•™ç¨‹äºŒï¼šä½¿ç”¨MySQL/PostgreSQL

#### ä½¿ç”¨MySQL

```bash
# å®‰è£…MySQLé©±åŠ¨
pip install pymysql

# é…ç½®config.yaml
database:
  type: "mysql"
  host: "localhost"
  port: 3306
  username: "root"
  password: "your_password"
  database: "model_finetune"
```

```bash
# åˆ›å»ºæ•°æ®åº“
mysql -u root -p
CREATE DATABASE model_finetune;
```

#### ä½¿ç”¨PostgreSQL

```bash
# å®‰è£…PostgreSQLé©±åŠ¨
pip install psycopg2-binary

# é…ç½®config.yaml
database:
  type: "postgresql"
  host: "localhost"
  port: 5432
  username: "postgres"
  password: "your_password"
  database: "model_finetune"
```

```bash
# åˆ›å»ºæ•°æ®åº“
psql -U postgres
CREATE DATABASE model_finetune;
```

### 5.3 æ•™ç¨‹ä¸‰ï¼šä½¿ç”¨å…¶ä»–LLM

#### DeepSeek

```yaml
llm:
  api_key: "${DEEPSEEK_API_KEY}"
  base_url: "https://api.deepseek.com/v1"
  model: "deepseek-chat"
```

#### é˜¿é‡Œé€šä¹‰åƒé—®

```yaml
llm:
  api_key: "${DASHSCOPE_API_KEY}"
  base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
  model: "qwen-turbo"
```

#### Ollama (æœ¬åœ°)

```yaml
llm:
  api_key: "ollama"  # ä»»æ„éç©ºå€¼
  base_url: "http://localhost:11434/v1"
  model: "llama3"
```

### 5.4 æ•™ç¨‹å››ï¼šæ¨¡å‹æ¨ç†æµ‹è¯•

è®­ç»ƒå®Œæˆåï¼Œæµ‹è¯•æ¨¡å‹æ•ˆæœï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# åŠ è½½åˆå¹¶åçš„æ¨¡å‹
model_path = "./output/my_dataset/merged"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)

# æ¨ç†æµ‹è¯•
prompt = "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## 6. å¸¸è§é—®é¢˜

### Q1: è§£æPDFæŠ¥é”™ï¼Ÿ

**é—®é¢˜**ï¼š`fitz` æ¨¡å—æœªå®‰è£…  
**è§£å†³**ï¼š
```bash
pip install pymupdf
```

### Q2: LLMè°ƒç”¨è¶…æ—¶ï¼Ÿ

**é—®é¢˜**ï¼šç½‘ç»œé—®é¢˜æˆ–APIé™æµ  
**è§£å†³**ï¼š
```yaml
# å¢åŠ è¶…æ—¶æ—¶é—´
llm:
  timeout: 60  # 60ç§’
```

### Q3: å†…å­˜ä¸è¶³ï¼Ÿ

**é—®é¢˜**ï¼šæ¨¡å‹å¤ªå¤§  
**è§£å†³**ï¼š
```yaml
# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
training:
  model_name: "Qwen/Qwen2.5-0.5B-Instruct"

# å‡å°æ‰¹æ¬¡å¤§å°
training:
  batch_size: 1
```

### Q4: è®­ç»ƒlossä¸ä¸‹é™ï¼Ÿ

**é—®é¢˜**ï¼šæ•°æ®è´¨é‡æˆ–è¶…å‚æ•°é—®é¢˜  
**è§£å†³**ï¼š
1. æ£€æŸ¥æ•°æ®è´¨é‡
2. è°ƒæ•´å­¦ä¹ ç‡ï¼š`0.0001` ~ `0.0003`
3. å¢åŠ è®­ç»ƒè½®æ•°

### Q5: ç”Ÿæˆçš„JSONæ ¼å¼é”™è¯¯ï¼Ÿ

**é—®é¢˜**ï¼šLLMå“åº”ä¸ç¨³å®š  
**è§£å†³**ï¼š
```yaml
# ä½¿ç”¨æ›´ç¨³å®šçš„æ¨¡å‹
llm:
  model: "gpt-4o-mini"

# æˆ–åœ¨promptä¸­å¼ºè°ƒJSONæ ¼å¼
```

### Q6: å¦‚ä½•å¢é‡æ›´æ–°æ•°æ®é›†ï¼Ÿ

```bash
# æ·»åŠ æ–°æ–‡æ¡£åé‡æ–°è§£æ
finetune parse ./documents my_dataset

# å·²å¤„ç†çš„æ–‡æ¡£ä¼šè‡ªåŠ¨è·³è¿‡
```

### Q7: å¦‚ä½•åˆ é™¤æ•°æ®é›†ï¼Ÿ

```bash
# æ¸…ç©ºæ•°æ®é›†ï¼ˆä¿ç•™é…ç½®ï¼‰
finetune clear my_dataset

# åˆ é™¤æ•´ä¸ªæ•°æ®åº“æ–‡ä»¶
rm ./data/datasets.db
```

### Q8: è®­ç»ƒé€Ÿåº¦æ…¢ï¼Ÿ

**å»ºè®®**ï¼š
1. ä½¿ç”¨GPUåŠ é€Ÿ
2. å‡å° `max_length`
3. å¢åŠ  `batch_size`
4. ä½¿ç”¨ `fp16` æ··åˆç²¾åº¦

### Q9: å¦‚ä½•æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼Ÿ

```bash
# è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨ checkpoints ç›®å½•
cat ./output/my_dataset/checkpoints/trainer_state.json
```

### Q10: æ€ä¹ˆåˆ‡æ¢ä¸åŒçš„æ•°æ®é›†ï¼Ÿ

```bash
# è§£æä¸ºæ–°æ•°æ®é›†
finetune parse ./documents new_dataset

# è®­ç»ƒæ–°æ•°æ®é›†
finetune train new_dataset
```

---

## 7. æœ€ä½³å®è·µ

### 7.1 æ•°æ®å‡†å¤‡

| å»ºè®® | è¯´æ˜ |
|------|------|
| âœ… æ¸…ç†æ— å…³å†…å®¹ | åˆ é™¤å¹¿å‘Šã€å¯¼èˆªæ ç­‰ |
| âœ… ç»Ÿä¸€æ ¼å¼ | å»ºè®®ä½¿ç”¨Markdown |
| âœ… æ§åˆ¶é•¿åº¦ | å•ä¸ªæ–‡ä»¶ä¸å®œè¿‡å¤§ |
| âœ… ä¸°å¯Œå†…å®¹ | åŒ…å«å¤šç§ä¸»é¢˜å’Œé—®ç­”ç±»å‹ |

### 7.2 æ•°æ®é‡å»ºè®®

| æ¨¡å‹è§„æ¨¡ | å»ºè®®æ•°æ®é‡ | è¯´æ˜ |
|----------|------------|------|
| 0.5B | 1K-5K | å°æ¨¡å‹æ•°æ®é‡ä¸å®œè¿‡å¤š |
| 1B | 5K-20K | ä¸­ç­‰è§„æ¨¡ |
| 3B+ | 20K+ | å¤§æ¨¡å‹å¯å¸æ”¶æ›´å¤šæ•°æ® |

### 7.3 è¶…å‚æ•°é€‰æ‹©

| å‚æ•° | æ¨èå€¼ | è°ƒæ•´å»ºè®® |
|------|--------|----------|
| learning_rate | 0.0002 | æ•°æ®é‡å¤§æ—¶é€‚å½“å‡å° |
| batch_size | 4-8 | æ˜¾å­˜å…è®¸æ—¶å¢å¤§ |
| epochs | 3-5 | æ ¹æ®lossæ”¶æ•›æƒ…å†µè°ƒæ•´ |
| lora.r | 8-16 | å¤æ‚ä»»åŠ¡ç”¨å¤§å€¼ |

### 7.4 æ•…éšœæ’æŸ¥

```bash
# 1. å¯ç”¨è¯¦ç»†æ—¥å¿—
finetune parse ./documents my_dataset -v

# 2. æ£€æŸ¥æ•°æ®åº“
sqlite3 ./data/datasets.db
sqlite> SELECT COUNT(*) FROM dataset_items;

# 3. éªŒè¯é…ç½®æ–‡ä»¶
python -c "from src.config import load_config; load_config('config.yaml')"
```

### 7.5 èµ„æºæ¸…ç†

```bash
# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -rf ./tmp/*
rm -rf ./data/cache/*

# æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹
rm -rf ./output/*/checkpoints/*
```

---

## é™„å½•

### A. å‘½ä»¤å‚è€ƒ

| å‘½ä»¤ | æè¿° |
|------|------|
| `finetune init` | åˆå§‹åŒ–é¡¹ç›® |
| `finetune parse <dir> <name>` | è§£ææ–‡æ¡£ |
| `finetune export <name>` | å¯¼å‡ºæ•°æ® |
| `finetune stats <name>` | æŸ¥çœ‹ç»Ÿè®¡ |
| `finetune train <name>` | è®­ç»ƒæ¨¡å‹ |
| `finetune merge <name> <base>` | åˆå¹¶æ¨¡å‹ |
| `finetune clear <name>` | æ¸…ç©ºæ•°æ® |

### B. é…ç½®æ–‡ä»¶æ¨¡æ¿

è§ `config.yaml`

### C. ç›¸å…³èµ„æº

- [é¡¹ç›®ä»“åº“](https://github.com/yourname/model-finetune-tool)
- [HuggingFace Hub](https://huggingface.co/models)
- [OpenAI APIæ–‡æ¡£](https://platform.openai.com/docs)
- [LoRAè®ºæ–‡](https://arxiv.org/abs/2106.09685)

---

**ç¥ä½¿ç”¨æ„‰å¿«ï¼** ğŸ‰
