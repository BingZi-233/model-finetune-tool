"""ä¸»ç¨‹åºå…¥å£"""

import hashlib
import json
import logging
import os
import platform
import re
import signal
import sys
import tempfile
from pathlib import Path
from typing import List, Optional

import click
from tqdm import tqdm

from .config import get_config, load_config, reload_config
from .parser import ParserManager
from .dataset import DatasetManager
from .llm import LLMClient, CacheManager
from .trainer import train_lora, merge_model, prepare_training_data

logger = logging.getLogger(__name__)

# é…ç½®æ—¥å¿—è¾“å‡ºåˆ° stderr
logging.basicConfig(level=logging.INFO, format="%(message)s", stream=sys.stderr)

# ============ å¹³å°æ£€æµ‹ ============
IS_WINDOWS = platform.system() == "Windows"
# IS_MACOS = platform.system() == "Darwin"  # å·²å®šä¹‰ä½†æœªä½¿ç”¨

# ============ å…¨å±€çŠ¶æ€ ============
_cli_verbose = False
_cli_quiet = False


# ============ ä¿¡å·å¤„ç† ============
def setup_signal_handlers():
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""

    def signal_handler(signum, frame):
        logger.info(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
        logger.info("\n[WARN] æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
        sys.exit(0)

    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    if not IS_WINDOWS:
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
    else:
        # Windows ä¸æ”¯æŒ SIGTERM
        signal.signal(signal.SIGINT, signal_handler)


def enable_verbose():
    """å¯ç”¨è¯¦ç»†è¾“å‡º"""
    global _cli_verbose
    _cli_verbose = True
    logging.getLogger().setLevel(logging.DEBUG)


def enable_quiet():
    """å¯ç”¨å®‰é™æ¨¡å¼"""
    global _cli_quiet
    _cli_quiet = True
    logging.getLogger().setLevel(logging.WARNING)


# ============ å¸¸é‡å®šä¹‰ ============
MIN_CHUNK_LENGTH = 100  # æœ€å°æ–‡æœ¬å—é•¿åº¦
MAX_TEXT_LENGTH = 100000  # æœ€å¤§è¾“å…¥æ–‡æœ¬é•¿åº¦ (100KB)
MAX_FILE_SIZE = 50 * 1024 * 1024  # æœ€å¤§æ–‡ä»¶å¤§å° (50MB)


# ============ è·¨å¹³å°å·¥å…·å‡½æ•° ============
def normalize_path(path: str) -> str:
    """è§„èŒƒåŒ–è·¯å¾„ï¼Œå¤„ç†ä¸åŒæ“ä½œç³»ç»Ÿçš„è·¯å¾„åˆ†éš”ç¬¦

    Args:
        path: åŸå§‹è·¯å¾„

    Returns:
        è§„èŒƒåŒ–åçš„è·¯å¾„
    """
    # å°†æ­£æ–œæ è½¬æ¢ä¸ºå½“å‰ç³»ç»Ÿçš„è·¯å¾„åˆ†éš”ç¬¦
    return path.replace("/", os.sep).replace("\\", os.sep)


def validate_path(path: str, base_dir: Optional[str] = None) -> str:
    """éªŒè¯è·¯å¾„å®‰å…¨æ€§ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»ï¼ˆè·¨å¹³å°ç‰ˆæœ¬ï¼‰

    Args:
        path: è¦éªŒè¯çš„è·¯å¾„
        base_dir: åŸºç¡€ç›®å½•ï¼Œé™åˆ¶è·¯å¾„åœ¨æ­¤ç›®å½•å†…

    Returns:
        éªŒè¯åçš„ç»å¯¹è·¯å¾„

    Raises:
        ValueError: è·¯å¾„ä¸åˆæ³•
    """
    # è§„èŒƒåŒ–è·¯å¾„åˆ†éš”ç¬¦
    path = normalize_path(path)

    # ä½¿ç”¨ Path å¯¹è±¡å¤„ç†è·¯å¾„ï¼ˆè·¨å¹³å°ï¼‰
    try:
        path_obj = Path(path)

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not path_obj.exists():
            raise ValueError(f"è·¯å¾„ä¸å­˜åœ¨: {path}")

        # è·å–ç»å¯¹è·¯å¾„
        clean_path = str(path_obj.resolve())

        # æ£€æŸ¥åŸºç¡€ç›®å½•é™åˆ¶
        if base_dir:
            base_dir = normalize_path(base_dir)
            base_path = Path(base_dir).resolve()

            # å°è¯•å¤šç§æ–¹å¼æ£€æŸ¥è·¯å¾„æ˜¯å¦åœ¨åŸºç¡€ç›®å½•å†…
            try:
                clean_path_obj = Path(clean_path)
                # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä»¥åŸºç¡€ç›®å½•å¼€å¤´
                if IS_WINDOWS:
                    # Windows ä¸åŒºåˆ†å¤§å°å†™
                    if (
                        not clean_path_obj.resolve().parts[: len(base_path.parts)]
                        == base_path.parts
                    ):
                        raise ValueError(f"è·¯å¾„è®¿é—®è¢«æ‹’ç»: {path}")
                else:
                    # Linux/Mac
                    if (
                        not clean_path_obj.resolve().parts[: len(base_path.parts)]
                        == base_path.parts
                    ):
                        raise ValueError(f"è·¯å¾„è®¿é—®è¢«æ‹’ç»: {path}")
            except ValueError:
                raise
            except Exception:
                # å¦‚æœ resolve() å¤±è´¥ï¼Œä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒ
                if not clean_path.startswith(str(base_path) + os.sep):
                    raise ValueError(f"è·¯å¾„è®¿é—®è¢«æ‹’ç»: {path}")

        return clean_path

    except OSError as e:
        raise ValueError(f"è·¯å¾„è®¿é—®é”™è¯¯: {e}")


def validate_file_size(file_path: str, max_size: int = MAX_FILE_SIZE) -> bool:
    """éªŒè¯æ–‡ä»¶å¤§å°

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        max_size: æœ€å¤§å…è®¸å¤§å°

    Returns:
        æ–‡ä»¶å¤§å°æ˜¯å¦åˆæ³•

    Raises:
        ValueError: æ–‡ä»¶è¿‡å¤§
    """
    # è§„èŒƒåŒ–è·¯å¾„
    file_path = normalize_path(file_path)

    if os.path.isfile(file_path):
        file_size = os.path.getsize(file_path)
        if file_size > max_size:
            raise ValueError(
                f"æ–‡ä»¶è¿‡å¤§: {file_path} ({file_size / 1024 / 1024:.1f}MB > "
                f"{max_size / 1024 / 1024:.1f}MB)"
            )
    return True


def validate_text_length(text: str, max_length: int = MAX_TEXT_LENGTH) -> None:
    """éªŒè¯æ–‡æœ¬é•¿åº¦

    Args:
        text: è¦éªŒè¯çš„æ–‡æœ¬
        max_length: æœ€å¤§å…è®¸é•¿åº¦

    Raises:
        ValueError: æ–‡æœ¬è¿‡é•¿
    """
    if len(text) > max_length:
        raise ValueError(f"è¾“å…¥æ–‡æœ¬è¿‡é•¿ ({len(text)} > {max_length} å­—ç¬¦)")


def get_default_config_path() -> str:
    """è·å–é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆè·¨å¹³å°ï¼‰

    Returns:
        é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„
    """
    return "config.yaml"


def get_data_dir() -> Path:
    """è·å–æ•°æ®ç›®å½•è·¯å¾„ï¼ˆè·¨å¹³å°ï¼‰

    Returns:
        æ•°æ®ç›®å½• Path å¯¹è±¡
    """
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)
    return data_dir


# ============ CLI å…¨å±€é€‰é¡¹ ============
def verbose_option(f):
    """æ·»åŠ  verbose é€‰é¡¹çš„è£…é¥°å™¨"""

    def callback(ctx, param, value):
        if value:
            enable_verbose()
        return value

    return click.option(
        "--verbose",
        "-v",
        is_flag=True,
        help="å¯ç”¨è¯¦ç»†è¾“å‡º",
        expose_value=False,
        callback=callback,
    )(f)


def quiet_option(f):
    """æ·»åŠ  quiet é€‰é¡¹çš„è£…é¥°å™¨"""

    def callback(ctx, param, value):
        if value:
            enable_quiet()
        return value

    return click.option(
        "--quiet",
        "-q",
        is_flag=True,
        help="å®‰é™æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º",
        expose_value=False,
        callback=callback,
    )(f)


# ============ CLI å‘½ä»¤ ============
@click.group()
@click.option("--config", "-c", default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
@click.option("--verbose", "-v", is_flag=True, help="å¯ç”¨è¯¦ç»†è¾“å‡º")
@click.option("--quiet", "-q", is_flag=True, help="å®‰é™æ¨¡å¼")
@click.pass_context
def cli(ctx, config, verbose, quiet):
    """æ¨¡å‹å¾®è°ƒå·¥å…·"""
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    setup_signal_handlers()

    # å¤„ç†å…¨å±€é€‰é¡¹
    if verbose and quiet:
        logger.info("[WARN] ä¸èƒ½åŒæ—¶ä½¿ç”¨ --verbose å’Œ --quiet")

    if verbose:
        enable_verbose()

    if quiet:
        enable_quiet()

    # ä¿å­˜é…ç½®è·¯å¾„åˆ°ä¸Šä¸‹æ–‡
    ctx.ensure_object(dict)
    ctx.obj["config"] = config
    ctx.obj["verbose"] = verbose
    ctx.obj["quiet"] = quiet

    logger.debug(f"é…ç½®è·¯å¾„: {config}")
    logger.debug(f"å¹³å°: {platform.system()}")


@cli.command()
@click.option("--config", "-c", default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
def init(config: str):
    """åˆå§‹åŒ–é¡¹ç›®"""
    config_path = Path(config)
    if config_path.exists():
        logger.info(f"é…ç½®æ–‡ä»¶å·²å­˜åœ¨: {config}")
    else:
        logger.info(f"åˆ›å»ºé…ç½®: {config}")


@cli.command()
@click.argument("input_dir")
@click.argument("dataset_name")
@click.option("--recursive/--no-recursive", default=True, help="é€’å½’è§£æå­ç›®å½•")
@click.option("--chunk-size", "-s", default=None, help="æ–‡æœ¬å—å¤§å° (100-10000)")
@click.option("--qa-pairs", "-n", default=3, help="æ¯æ®µæ–‡æœ¬ç”Ÿæˆçš„QAå¯¹æ•°é‡ (1-20)")
@click.pass_context
def parse(
    ctx,
    input_dir: str,
    dataset_name: str,
    recursive: bool,
    chunk_size: Optional[int],
    qa_pairs: int,
):
    """è§£ææ–‡æ¡£å¹¶ç”Ÿæˆæ•°æ®é›†"""
    # è·å–å…¨å±€é…ç½®è·¯å¾„
    config_path = ctx.obj.get("config", "config.yaml")

    logger.info("=" * 60)
    logger.info("ğŸš€ å¼€å§‹è§£ææ–‡æ¡£å¹¶ç”Ÿæˆæ•°æ®é›†")
    logger.info("=" * 60)
    logger.info(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
    logger.info(f"ğŸ“Š æ•°æ®é›†åç§°: {dataset_name}")
    logger.info(f"ğŸ”„ é€’å½’æ‰«æ: {'æ˜¯' if recursive else 'å¦'}")

    # éªŒè¯å‚æ•°
    if chunk_size is not None:
        if chunk_size < 100 or chunk_size > 10000:
            raise click.BadParameter(
                f"chunk_size å¿…é¡»åœ¨ 100-10000 ä¹‹é—´", param_hint="--chunk-size"
            )

    if qa_pairs < 1 or qa_pairs > 20:
        raise click.BadParameter(f"qa-pairs å¿…é¡»åœ¨ 1-20 ä¹‹é—´", param_hint="--qa-pairs")

    # éªŒè¯ dataset_name
    if not dataset_name or not dataset_name.strip():
        raise click.BadParameter("dataset_name ä¸èƒ½ä¸ºç©º", param_hint="DATASET_NAME")

    # éªŒè¯è·¯å¾„å®‰å…¨æ€§
    try:
        input_dir = validate_path(input_dir)
    except ValueError as e:
        print(f"[ERROR] {e}", file=sys.stderr, flush=True)
        return

    try:
        cfg = load_config(config_path)
    except Exception as e:
        print(f"[ERROR] åŠ è½½é…ç½®å¤±è´¥: {e}", file=sys.stderr, flush=True)
        return

    if chunk_size:
        cfg.datasets.chunk_size = chunk_size

    logger.info(f"ğŸ“ æ–‡æœ¬å—å¤§å°: {cfg.datasets.chunk_size}")
    logger.info(f"â“ æ¯ä¸ªæ–‡æœ¬å—ç”ŸæˆQAå¯¹æ•°é‡: {qa_pairs}")
    logger.info(f"ğŸ¤– LLMæ¨¡å‹: {cfg.llm.model}")
    logger.info("-" * 60)

    # åˆå§‹åŒ–ç®¡ç†å™¨
    parser = ParserManager()
    db_manager = DatasetManager()
    llm_client = LLMClient()

    # è§£ææ–‡æ¡£
    logger.info(f"ğŸ“‚ å¼€å§‹æ‰«ææ–‡æ¡£ç›®å½•...")

    try:
        documents = parser.parse_directory(input_dir, recursive)
    except (OSError, IOError) as e:
        logger.error(f"[ERROR] è¯»å–æ–‡æ¡£ç›®å½•å¤±è´¥: {e}")
        return
    except ValueError as e:
        logger.error(f"[ERROR] æ–‡æ¡£æ ¼å¼é”™è¯¯: {e}")
        return
    except Exception as e:
        logger.error(f"[ERROR] è§£ææ–‡æ¡£å¤±è´¥: {e}")
        logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:", exc_info=True)
        return

    if not documents:
        logger.warning("[WARN] æ²¡æœ‰æ‰¾åˆ°å¯è§£æçš„æ–‡æ¡£")
        return

    logger.info("-" * 60)
    logger.info(f"[OK] æ‰«æå®Œæˆ! å‘ç° {len(documents)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")

    # ç»Ÿè®¡æ€»æ®µè½æ•°
    total_paragraphs = sum(len(paras) for paras in documents.values())
    logger.info(f"ğŸ“ æ€»æ®µè½æ•°: {total_paragraphs}")

    # å¤„ç†æ¯ä¸ªæ–‡æ¡£
    total_items = 0
    skipped_files = 0
    error_files = []
    total_chunks = 0

    logger.info("-" * 60)
    logger.info("ğŸ”„ å¼€å§‹ç”ŸæˆQAå¯¹...")
    logger.info("-" * 60)

    for file_path, paragraphs in tqdm(documents.items(), desc="ğŸ”„ å¤„ç†æ–‡æ¡£"):
        # éªŒè¯æ–‡ä»¶å¤§å°
        try:
            validate_file_size(file_path)
        except ValueError as e:
            logger.warning(f"[WARN] è·³è¿‡å¤§æ–‡ä»¶: {e}")
            continue

        # è®¡ç®—å†…å®¹hash
        content_hash = hashlib.md5("".join(paragraphs).encode()).hexdigest()

        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        if db_manager.document_exists(file_path, content_hash):
            skipped_files += 1
            continue

        # æ·»åŠ æ–‡æ¡£è®°å½•
        doc_id = db_manager.add_document(file_path, content_hash)

        # åˆ‡åˆ†æ–‡æœ¬
        chunks = []
        for i, para in enumerate(paragraphs):
            if len(para) > cfg.datasets.chunk_size:
                # é•¿æ–‡æœ¬åˆ‡åˆ†æˆå°å—
                for j in range(
                    0, len(para), cfg.datasets.chunk_size - cfg.datasets.chunk_overlap
                ):
                    chunk = para[j : j + cfg.datasets.chunk_size]
                    if len(chunk) > MIN_CHUNK_LENGTH:  # ä½¿ç”¨å¸¸é‡
                        chunks.append(chunk)
            else:
                if len(para) > MIN_CHUNK_LENGTH:  # ä½¿ç”¨å¸¸é‡
                    chunks.append(para)

        total_chunks += len(chunks)
        logger.info(
            f"ğŸ“„ [{Path(file_path).name}] {len(paragraphs)} æ®µè½ â†’ {len(chunks)} æ–‡æœ¬å—"
        )

        # ç”ŸæˆQAå¯¹
        for chunk_idx, chunk in enumerate(chunks):
            # éªŒè¯æ–‡æœ¬é•¿åº¦
            try:
                validate_text_length(chunk)
            except ValueError as e:
                logger.warning(f"[WARN] è·³è¿‡è¿‡é•¿æ–‡æœ¬å—: {e}")
                continue

            # è¾“å‡ºå½“å‰å¤„ç†è¿›åº¦åˆ° stderr
            file_name = Path(file_path).name
            total_chunks_processed = sum(
                1 for f, p in documents.items() for _ in range(min(len(p), 100))
            )  # ä¼°ç®—
            logger.info(
                f"ğŸ”„ å¤„ç†ä¸­: [{file_name}] {chunk_idx + 1}/{len(chunks)} æ–‡æœ¬å—..."
            )

            try:
                # ç”ŸæˆQAå¯¹ï¼ˆä¼šæ˜¾ç¤ºLLMå“åº”ï¼‰
                qa = llm_client.generate_qa_pairs(chunk, qa_pairs)

                # è¾“å‡ºç”Ÿæˆç»“æœ
                if qa:
                    logger.info(
                        f"   [OK] ç”Ÿæˆ {len(qa)} ä¸ªQAå¯¹ (æ€»è®¡: {total_items + len(qa)})"
                    )
                else:
                    logger.warning(f"   [WARN] æœªç”Ÿæˆä»»ä½•QAå¯¹")

                for qa_item in qa:
                    db_manager.add_dataset_item(
                        dataset_name=dataset_name,
                        instruction=qa_item.get("instruction", ""),
                        input_=qa_item.get("input", ""),
                        output=qa_item.get("output", ""),
                        document_id=doc_id,
                        chunk_index=chunk_idx,
                        source_file=file_path,
                    )
                    total_items += 1
            except Exception as e:
                error_files.append((file_path, str(e)))
                logger.error(f"   [ERROR] ç”Ÿæˆå¤±è´¥: {e}")
                logger.error(f"ç”ŸæˆQAå¤±è´¥: {e}")
                continue

        # æ¯ä¸ªæ–‡ä»¶å¤„ç†å®Œæˆåè¾“å‡ºæ€»ç»“
        logger.info(
            f"\n[OK] [{file_name}] å¤„ç†å®Œæˆ! æœ¬æ–‡ä»¶ç”Ÿæˆ {sum(1 for _ in chunks)} ä¸ªæ–‡æœ¬å—"
        )

    logger.info("-" * 60)
    logger.info("ğŸ“Š å¤„ç†å®Œæˆ! ç»Ÿè®¡ä¿¡æ¯:")
    logger.info("=" * 60)
    logger.info(f"[OK] æˆåŠŸå¤„ç†æ–‡æ¡£: {len(documents) - skipped_files - len(error_files)}")
    logger.info(f"ğŸ“Œ è·³è¿‡å·²å¤„ç†æ–‡æ¡£: {skipped_files}")
    if error_files:
        logger.error(f"[ERROR] å¤„ç†å¤±è´¥æ–‡æ¡£: {len(error_files)}")
    logger.info(f"ğŸ“¦ æ€»æ–‡æœ¬å—æ•°: {total_chunks}")
    logger.info(f"ğŸ¯ ç”ŸæˆQAå¯¹æ€»æ•°: {total_items}")
    logger.info(f"ğŸ“ æ•°æ®é›†: {dataset_name}")
    logger.info("=" * 60)


@cli.command()
@click.argument("dataset_name")
@click.option(
    "--format", "output_format", type=click.Choice(["jsonl", "json"]), default="jsonl"
)
@click.option("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
def export(dataset_name: str, output_format: str, output: Optional[str]):
    """å¯¼å‡ºæ•°æ®é›†"""
    db_manager = DatasetManager()

    if output is None:
        output = f"{dataset_name}.{output_format}"

    if output_format == "jsonl":
        count = db_manager.save_to_jsonl(dataset_name, output)
        logger.info(f"[OK] å¯¼å‡º {count} æ¡æ•°æ®åˆ° {output}")
    else:
        data = db_manager.export_dataset(dataset_name)
        with open(output, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"[OK] å¯¼å‡º {len(data)} æ¡æ•°æ®åˆ° {output}")


@cli.command()
@click.argument("dataset_name")
def stats(dataset_name: str):
    """æŸ¥çœ‹æ•°æ®é›†ç»Ÿè®¡"""
    db_manager = DatasetManager()
    stats = db_manager.get_dataset_stats(dataset_name)

    logger.info(f"æ•°æ®é›†: {stats['dataset_name']}")
    logger.info(f"æ€»æ¡ç›®: {stats['total_items']}")


@cli.command()
@click.argument("dataset_name")
@click.option("--model", "-m", help="æ¨¡å‹åç§°")
@click.option("--epochs", "-e", default=None, help="è®­ç»ƒè½®æ•°")
@click.option("--batch-size", "-b", default=None, help="æ‰¹æ¬¡å¤§å°")
@click.option("--max-length", "-l", default=None, help="æœ€å¤§åºåˆ—é•¿åº¦")
def train(
    dataset_name: str,
    model: Optional[str],
    epochs: Optional[int],
    batch_size: Optional[int],
    max_length: Optional[int],
):
    """è®­ç»ƒæ¨¡å‹"""
    # éªŒè¯å‚æ•°
    if epochs is not None and (epochs < 1 or epochs > 100):
        raise click.BadParameter(f"epochs å¿…é¡»åœ¨ 1-100 ä¹‹é—´", param_hint="--epochs")

    if batch_size is not None and (batch_size < 1 or batch_size > 64):
        raise click.BadParameter(f"batch_size å¿…é¡»åœ¨ 1-64 ä¹‹é—´", param_hint="--batch-size")

    if max_length is not None and (max_length < 128 or max_length > 8192):
        raise click.BadParameter(f"max_length å¿…é¡»åœ¨ 128-8192 ä¹‹é—´", param_hint="--max-length")

    cfg = get_config()

    model_name = model or cfg.training.model_name
    epochs = epochs or cfg.training.epochs
    batch_size = batch_size or cfg.training.batch_size
    max_length = max_length or cfg.training.max_length

    # å¯¼å‡ºæ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as tmp:
        data_path = tmp.name
    db_manager = DatasetManager()
    db_manager.save_to_jsonl(dataset_name, data_path)

    # å‡†å¤‡æ•°æ®
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as tmp:
        prepared_path = tmp.name
    prepare_training_data(data_path, prepared_path)

    output_dir = f"./output/{dataset_name}"

    logger.info(f"å¼€å§‹è®­ç»ƒæ¨¡å‹: {model_name}")

    train_lora(
        model_name=model_name,
        data_path=prepared_path,
        output_dir=output_dir,
        batch_size=batch_size,
        epochs=epochs,
        max_length=max_length,
    )

    logger.info(f"[OK] è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ°: {output_dir}")


@cli.command()
@click.argument("dataset_name")
@click.argument("base_model")
@click.option("--output", "-o", help="è¾“å‡ºè·¯å¾„")
def merge(dataset_name: str, base_model: str, output: Optional[str]):
    """åˆå¹¶æ¨¡å‹"""
    lora_path = f"./output/{dataset_name}/lora_model"

    if not Path(lora_path).exists():
        logger.error(f"[ERROR] LoRAæ¨¡å‹ä¸å­˜åœ¨: {lora_path}")
        return

    output_path = output or f"./output/{dataset_name}/merged"

    merge_model(base_model, lora_path, output_path)
    logger.info(f"[OK] æ¨¡å‹å·²åˆå¹¶åˆ°: {output_path}")


@cli.command()
@click.argument("dataset_name")
def clear(dataset_name: str):
    """æ¸…ç©ºæ•°æ®é›†"""
    db_manager = DatasetManager()
    db_manager.clear_dataset(dataset_name)
    logger.info(f"[OK] å·²æ¸…ç©ºæ•°æ®é›†: {dataset_name}")


def main():
    """ä¸»å…¥å£"""
    cli()


if __name__ == "__main__":
    main()
